{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.5' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "Viterbi Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m treebank\n\u001b[1;32m      3\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtreebank\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m corpus \u001b[38;5;241m=\u001b[39m treebank\u001b[38;5;241m.\u001b[39mwords()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "nltk.download('treebank')\n",
    "\n",
    "corpus = treebank.words()\n",
    "print(corpus[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words = treebank.tagged_words()\n",
    "print(tagged_words[:40])\n",
    "# show all the distinct tags\n",
    "tags = set([tag for word, tag in tagged_words])\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tags we plan to use are \n",
    "- NN (Noun) {All forms of Noun are represented by NN}\n",
    "- VB (Verb) {All forms of Verb are represented by VB}\n",
    "- JJ (Adjective) {All forms of Adjective are represented by JJ}\n",
    "- RB (Adverb)\n",
    "- TO (to)\n",
    "- DT (Determiner)\n",
    "- PRP (Pronoun)\n",
    "- other (all the remaining tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'NN':0 , 'VB':1 , 'JJ':2 , 'RB':3 , 'TO':4 , 'DT':5 , 'PRP':6 ,'other':7}\n",
    "def get_tag(tag):\n",
    "    if tag.startswith('NN'):\n",
    "        return dict['NN']\n",
    "    elif tag.startswith('VB'):\n",
    "        return dict['VB']\n",
    "    elif tag.startswith('JJ'):\n",
    "        return dict['JJ']\n",
    "    elif tag.startswith('RB'):\n",
    "        return dict['RB']\n",
    "    elif tag.startswith('TO'):\n",
    "        return dict['TO']\n",
    "    elif tag.startswith('DT'):\n",
    "        return dict['DT']\n",
    "    elif tag.startswith('PRP'):\n",
    "        return dict['PRP']\n",
    "    else:\n",
    "        return dict['other']\n",
    "\n",
    "tagged_words = [(word, get_tag(tag)) for word, tag in tagged_words]\n",
    "print(tagged_words[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_tags = 8\n",
    "n_words = len(set(corpus))\n",
    "word_to_index = {word: i for i, word in enumerate(set(corpus))}\n",
    "emission_matrix = np.zeros((n_tags, n_words+1))\n",
    "transition_matrix = np.zeros((n_tags, n_tags))\n",
    "\n",
    "# count the occurences of each tag\n",
    "tag_counts = np.zeros(n_tags)\n",
    "prev_tag = None\n",
    "for word, tag in tagged_words:\n",
    "    tag_counts[tag] += 1\n",
    "    emission_matrix[tag, word_to_index[word]] += 1\n",
    "    if prev_tag is not None:\n",
    "        transition_matrix[prev_tag, tag] += 1\n",
    "    prev_tag = tag\n",
    "    \n",
    "    \n",
    "# convert the counts to their log probabilities\n",
    "emission_matrix = np.log(emission_matrix + 1) - np.log(tag_counts.reshape(-1, 1) + n_words)\n",
    "transition_matrix = np.log(transition_matrix + 1) - np.log(tag_counts + n_tags)\n",
    "\n",
    "print(emission_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing the Viterbi algorithm\n",
    "\n",
    "def viterbi(words, emission_matrix, transition_matrix):\n",
    "    n_tags, n_words = emission_matrix.shape\n",
    "    n = len(words)\n",
    "    scores = np.zeros((n_tags, n))\n",
    "    backpointers = np.zeros((n_tags, n), dtype=int)\n",
    "    \n",
    "    # initialization\n",
    "    scores[:, 0] = transition_matrix[-1, :] + emission_matrix[:, word_to_index[words[0]]]\n",
    "    \n",
    "    # Dynamic programming part \n",
    "    for i in range(1, n):\n",
    "        for j in range(n_tags):\n",
    "            score = scores[:, i - 1] + transition_matrix[:, j] + emission_matrix[j, word_to_index.get(words[i],n_words-1)]\n",
    "            scores[j, i] = np.max(score)\n",
    "            backpointers[j, i] = np.argmax(score)\n",
    "    \n",
    "    # retrieving the sequence of tags\n",
    "    best_tag = np.argmax(scores[:, n - 1])\n",
    "    tags = [best_tag]\n",
    "    for i in range(n - 1, 0, -1):\n",
    "        best_tag = backpointers[best_tag, i]\n",
    "        tags.append(best_tag)\n",
    "    \n",
    "    return list(reversed(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = \"I want to go home\"\n",
    "words = str.split()\n",
    "tags = viterbi(words, emission_matrix, transition_matrix)\n",
    "print(list(zip(words, [list(dict.keys())[tag] for tag in tags])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "Vanilla Emotion Recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      2\u001b[0m ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdair-ai/emotion\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m ds\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"dair-ai/emotion\",\"split\")\n",
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do tfidf vectorization of the dataset ds\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(ds['train']['text'])\n",
    "Y = ds['train']['label']\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use an svm to build a classifier for this dataset\n",
    "from sklearn import svm\n",
    "\n",
    "SVM1 = svm.SVC()\n",
    "SVM1.fit(X, Y)\n",
    "\n",
    "# test the classifier on the test set\n",
    "X_test = vectorizer.transform(ds['test']['text'])\n",
    "Y_pred = SVM1.predict(X_test)\n",
    "\n",
    "# calculate the accuracy of the classifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(ds['test']['label'], Y_pred)\n",
    "print(accuracy)\n",
    "\n",
    "# calculate the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(ds['test']['label'], Y_pred)\n",
    "print(cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# use the viterbi algorithm to tag the words in the dataset\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mds\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[1;32m      4\u001b[0m     words \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m      5\u001b[0m     tags \u001b[38;5;241m=\u001b[39m viterbi(words, emission_matrix, transition_matrix)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "# use the viterbi algorithm to tag the words in the dataset\n",
    "\n",
    "for i in range(len(ds['train']['text'])):\n",
    "    \n",
    "\n",
    "X_train = vectorizer.transform(ds['train']['text'])\n",
    "Y_train = ds['train']['label']\n",
    "\n",
    "SVM2 = svm.SVC()\n",
    "SVM2.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(ds['test']['text'])\n",
    "Y_pred = SVM2.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(ds['test']['label'], Y_pred)\n",
    "print(accuracy)\n",
    "\n",
    "cm = confusion_matrix(ds['test']['label'], Y_pred)\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
